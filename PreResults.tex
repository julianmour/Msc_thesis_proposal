%! Author = julianmour
%! Date = 01/05/2023

\section{Preliminary Results}
We evaluated our preliminary approach on the DOUBLE-MNIST test dataset, consisting of images showing two digits. The multi-classifier's goal is to return the correct two digits. % from ten classes;
%$C = \{0, 1, \ldots,9\}$ (the 10 different digits), each image classified to two different classes (contains two different digits).
An example of an image is shown in Figure~\ref{fig:double-mnist-sample}. %, where the digit $4$ is the target object and the digit $9$ is the non-target object.
\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{3.png}
    \caption{DOUBLE-MNIST sample}
    \label{fig:double-mnist-sample}
\end{figure}
We ran our algorithm on three different CNN multi-label DOUBLE-MNIST classifiers, all with the same architecture\Dana{describe it and how many neurons} but a different training procedure:
%While the three of them solve the same classification problem, they differ in their training process:
\begin{itemize}
    \item Without defense. %- This network is trained regularly on the original DOUBLE-MNIST training dataset, without additional processing done to the training dataset.
      \item With an $L_0$ defense: this defense relies on the following data augmentation.
    Before forwarding a training sample to the network, we add random noise to the image in the form of a black rectangle.
        \item With an $L_{\infty}$ defense: using the Projected Gradient Descent (PGD) defense~\cite{PGD}.
    This defense also involves training the model with adversarial examples, but unlike the $L_0$ defense, the added perturbations are a small value and can be anywhere in the input.
    %The PGD generated adversarial examples are achieved by trying to increase the model's loss function as much as possible.
    %Therefore, training the model with such adversarial examples should make the network more robust.
\end{itemize} 

  In Figure~\ref{fig:No defense} we present results of our program ran on this classifier and a single image, as a heatmap representing the epsilons per each layer.
    \begin{figure}
         \centering
         \begin{subfigure}[b]{0.4\textwidth}
             \centering
             \includegraphics[width=\textwidth]{no_defense_fixed_weights.png}
             \caption{fixed weights}
             \label{sub-fig:No defense FW}
         \end{subfigure}
         \hfill
         \begin{subfigure}[b]{0.4\textwidth}
             \centering
             \includegraphics[width=\textwidth]{no_defense_sensitivity_weights.png}
             \caption{sensitivity weights}
             \label{sub-fig:No defense SW}
         \end{subfigure}
         \caption{No Defense}
         \label{fig:No defense}
     \end{figure}
     In Figure~\ref{fig:L0 defense} we present results of our program ran on this classifier and a single image, as a heatmap representing the epsilons per each layer.
    \begin{figure}
         \centering
         \begin{subfigure}[b]{0.4\textwidth}
             \centering
             \includegraphics[width=\textwidth]{l0_defense_fixed_weights.png}
             \caption{fixed weights}
             \label{sub-fig:L0 defense FW}
         \end{subfigure}
         \hfill
         \begin{subfigure}[b]{0.4\textwidth}
             \centering
             \includegraphics[width=\textwidth]{l0_defense_sensitivity_weights.png}
             \caption{sensitivity weights}
             \label{sub-fig:L0 defense SW}
         \end{subfigure}
         \caption{$L_0$ Defense}
         \label{fig:L0 defense}
    \end{figure}