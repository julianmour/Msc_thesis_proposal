@inproceedings{roelofs2019meta,
  title={A meta-analysis of overfitting in machine learning},
  author={Roelofs, Rebecca and Fridovich-Keil, Sara and Miller, John and Shankar, Vaishaal and Hardt, Moritz and Recht, Benjamin and Schmidt, Ludwig},
  booktitle={Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  pages={9179--9189},
  year={2019}
}

@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization. arXiv preprint (In ICLR 2017)},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}

@inproceedings{garg2021ratt,
  title={Ratt: Leveraging unlabeled data to guarantee generalization},
  author={Garg, Saurabh and Balakrishnan, Sivaraman and Kolter, Zico and Lipton, Zachary},
  booktitle={International Conference on Machine Learning},
  pages={3598--3609},
  year={2021},
  organization={PMLR}
}

@incollection{hoeffding1994probability,
  title={Probability inequalities for sums of bounded random variables},
  author={Hoeffding, Wassily},
  booktitle={The Collected Works of Wassily Hoeffding},
  pages={409--426},
  year={1994},
  publisher={Springer}
}

@article{bardenet2015concentration,
  title={Concentration inequalities for sampling without replacement},
  author={Bardenet, R{\'e}mi and Maillard, Odalric-Ambrym and others},
  journal={Bernoulli},
  volume={21},
  number={3},
  pages={1361--1385},
  year={2015},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@article{OnePixelAttack,
   title={One Pixel Attack for Fooling Deep Neural Networks},
   volume={23},
   ISSN={1941-0026},
   url={http://dx.doi.org/10.1109/TEVC.2019.2890858},
   DOI={10.1109/tevc.2019.2890858},
   number={5},
   journal={IEEE Transactions on Evolutionary Computation},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Su, Jiawei and Vargas, Danilo Vasconcellos and Sakurai, Kouichi},
   year={2019},
   month={Oct},
   pages={828–841} }

@misc{szegedy2014intriguing,
      title={Intriguing properties of neural networks},
      author={Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
      year={2014},
      eprint={1312.6199},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{taxonomy,
author = {Wu, Junde and Fu, Rao},
year = {2019},
month = {08},
pages = {},
title = {Universal, transferable and targeted adversarial attacks}
}

@misc{he2015deep,
      title={Deep Residual Learning for Image Recognition},
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@incollection{GOLDBERG199169,
title = {A Comparative Analysis of Selection Schemes Used in Genetic Algorithms},
editor = {GREGORY J.E. RAWLINS},
series = {Foundations of Genetic Algorithms},
publisher = {Elsevier},
volume = {1},
pages = {69-93},
year = {1991},
issn = {1081-6593},
doi = {https://doi.org/10.1016/B978-0-08-050684-5.50008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780080506845500082},
author = {David E. Goldberg and Kalyanmoy Deb}
}

@misc{real2019regularized,
      title={Regularized Evolution for Image Classifier Architecture Search},
      author={Esteban Real and Alok Aggarwal and Yanping Huang and Quoc V Le},
      year={2019},
      eprint={1802.01548},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}
@misc{alatalo2021chromatic,
      title={Chromatic and spatial analysis of one-pixel attacks against an image classifier},
      author={Janne Alatalo and Joni Korpihalkola and Tuomo Sipola and Tero Kokkonen},
      year={2021},
      eprint={2105.13771},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{vargas2019understanding,
      title={Understanding the One-Pixel Attack: Propagation Maps and Locality Analysis},
      author={Danilo Vasconcellos Vargas and Jiawei Su},
      year={2019},
      eprint={1902.02947},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{krizhevsky2014weird,
      title={One weird trick for parallelizing convolutional neural networks},
      author={Alex Krizhevsky},
      year={2014},
      eprint={1404.5997},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@misc{vaswani2017attention,
      title={Attention Is All You Need},
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ribeiro2016why,
      title={"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
      author={Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
      year={2016},
      eprint={1602.04938},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{goodfellow2015explaining,
      title={Explaining and Harnessing Adversarial Examples},
      author={Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
      year={2015},
      eprint={1412.6572},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{papernot2015limitations,
      title={The Limitations of Deep Learning in Adversarial Settings},
      author={Nicolas Papernot and Patrick McDaniel and Somesh Jha and Matt Fredrikson and Z. Berkay Celik and Ananthram Swami},
      year={2015},
      eprint={1511.07528},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{moosavidezfooli2016deepfool,
      title={DeepFool: a simple and accurate method to fool deep neural networks},
      author={Seyed-Mohsen Moosavi-Dezfooli and Alhussein Fawzi and Pascal Frossard},
      year={2016},
      eprint={1511.04599},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{BlackBox1,
  author    = {Nicolas Papernot and
               Patrick D. McDaniel and
               Ian J. Goodfellow and
               Somesh Jha and
               Z. Berkay Celik and
               Ananthram Swami},
  title     = {Practical Black-Box Attacks against Deep Learning Systems using Adversarial
               Examples},
  journal   = {CoRR},
  volume    = {abs/1602.02697},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.02697},
  eprinttype = {arXiv},
  eprint    = {1602.02697},
  timestamp = {Mon, 13 Aug 2018 16:49:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/PapernotMGJCS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{BlackBox2,
  author    = {Hao Qiu and
               Leonardo Lucio Custode and
               Giovanni Iacca},
  title     = {Black-box adversarial attacks using Evolution Strategies},
  journal   = {CoRR},
  volume    = {abs/2104.15064},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.15064},
  eprinttype = {arXiv},
  eprint    = {2104.15064},
  timestamp = {Tue, 04 May 2021 15:12:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-15064.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{BlackBox3,
  author    = {Andrew Ilyas and
               Logan Engstrom and
               Anish Athalye and
               Jessy Lin},
  title     = {Black-box Adversarial Attacks with Limited Queries and Information},
  journal   = {CoRR},
  volume    = {abs/1804.08598},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.08598},
  eprinttype = {arXiv},
  eprint    = {1804.08598},
  timestamp = {Mon, 13 Aug 2018 16:47:47 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-08598.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{BlackBox4,
  author    = {Laurent Meunier and
               Jamal Atif and
               Olivier Teytaud},
  title     = {Yet another but more efficient black-box adversarial attack: tiling
               and evolution strategies},
  journal   = {CoRR},
  volume    = {abs/1910.02244},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.02244},
  eprinttype = {arXiv},
  eprint    = {1910.02244},
  timestamp = {Wed, 09 Oct 2019 14:07:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-02244.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{moosavidezfooli2017universal,
      title={Universal adversarial perturbations},
      author={Seyed-Mohsen Moosavi-Dezfooli and Alhussein Fawzi and Omar Fawzi and Pascal Frossard},
      year={2017},
      eprint={1610.08401},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{khrulkov2017art,
      title={Art of singular vectors and universal adversarial perturbations},
      author={Valentin Khrulkov and Ivan Oseledets},
      year={2017},
      eprint={1709.03582},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{mopuri2018nag,
      title={NAG: Network for Adversary Generation},
      author={Konda Reddy Mopuri and Utkarsh Ojha and Utsav Garg and R. Venkatesh Babu},
      year={2018},
      eprint={1712.03390},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inbook{inbook,
author = {Gulwani, S.},
year = {2016},
month = {04},
pages = {137-158},
title = {Programming by examples (and its applications in data wrangling)},
doi = {10.3233/978-1-61499-627-9-137}
}

@inproceedings{d39antoni2016qlose,
author = {D'Antoni, Loris and Samanta, Roopsha and Singh, Rishabh},
title = {Qlose: Program Repair with Quantiative Objectives},
booktitle = {27th International Conference on Computer Aided Verification (CAV 2016)},
year = {2016},
month = {July},
}

@article{10.1145/2666356.2594321,
author = {Raychev, Veselin and Vechev, Martin and Yahav, Eran},
title = {Code Completion with Statistical Language Models},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/2666356.2594321},
doi = {10.1145/2666356.2594321},
abstract = {We address the problem of synthesizing code completions for programs using APIs. Given a program with holes, we synthesize completions for holes with the most likely sequences of method calls.Our main idea is to reduce the problem of code completion to a natural-language processing problem of predicting probabilities of sentences. We design a simple and scalable static analysis that extracts sequences of method calls from a large codebase, and index these into a statistical language model. We then employ the language model to find the highest ranked sentences, and use them to synthesize a code completion. Our approach is able to synthesize sequences of calls across multiple objects together with their arguments.Experiments show that our approach is fast and effective. Virtually all computed completions typecheck, and the desired completion appears in the top 3 results in 90% of the cases.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {419–428},
numpages = {10}
}

@inproceedings{10.1145/2594291.2594321,
author = {Raychev, Veselin and Vechev, Martin and Yahav, Eran},
title = {Code Completion with Statistical Language Models},
year = {2014},
isbn = {9781450327848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2594291.2594321},
doi = {10.1145/2594291.2594321},
pages = {419–428},
numpages = {10},
location = {Edinburgh, United Kingdom},
series = {PLDI '14}
}

@inproceedings{nori2015efficient,
author = {Nori, Aditya and Ozair, Sherjil and Rajamani, Sriram and Vijaykeerthy,  Deepak and },
title = {Efficient Synthesis of Probabilistic Programs},
booktitle = {Programming Language Design and Implementation (PLDI)},
year = {2015},
month = {June},
publisher = {ACM - Association for Computing Machinery},
url = {https://www.microsoft.com/en-us/research/publication/efficient-synthesis-of-probabilistic-programs/},
edition = {Programming Language Design and Implementation (PLDI)},
}

@article{10.1145/36205.36194,
author = {Massalin, Henry},
title = {Superoptimizer: A Look at the Smallest Program},
year = {1987},
issue_date = {Oct. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/36205.36194},
doi = {10.1145/36205.36194},
abstract = {Given an instruction set, the superoptimizer finds the shortest program to compute a function. Startling programs have been generated, many of them engaging in convoluted bit-fiddling bearing little resemblance to the source programs which defined the functions. The key idea in the superoptimizer is a probabilistic test that makes exhaustive searches practical for programs of useful size. The search space is defined by the processor's instruction set, which may include the whole set, but it is typically restricted to a subset. By constraining the instructions and observing the effect on the output program, one can gain insight into the design of instruction sets. In addition, superoptimized programs may be used by peephole optimizers to improve the quality of generated code, or by assembly language programmers to improve manually written code.},
journal = {SIGPLAN Not.},
month = {oct},
pages = {122–126},
numpages = {5}
}

@inproceedings{10.1145/36206.36194,
author = {Massalin, Henry},
title = {Superoptimizer: A Look at the Smallest Program},
year = {1987},
isbn = {0818608056},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
url = {https://doi.org/10.1145/36206.36194},
doi = {10.1145/36206.36194},
pages = {122–126},
numpages = {5},
location = {Palo Alto, California, USA},
series = {ASPLOS II}
}

@INPROCEEDINGS{6679385,
  author={Alur, Rajeev and Bodik, Rastislav and Juniwal, Garvit and Martin, Milo M. K. and Raghothaman, Mukund and Seshia, Sanjit A. and Singh, Rishabh and Solar-Lezama, Armando and Torlak, Emina and Udupa, Abhishek},
  booktitle={2013 Formal Methods in Computer-Aided Design},
  title={Syntax-guided synthesis},
  year={2013},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/FMCAD.2013.6679385}}

@article{CSP,
author = {Ghédira, K. and Dubuisson, B.},
year = {2013},
month = {02},
pages = {},
title = {Constraint Satisfaction Problems: CSP Formalisms and Techniques},
journal = {Constraint Satisfaction Problems: CSP Formalisms and Techniques},
doi = {10.1002/9781118574522}
}

@misc{schkufza2012stochastic,
      title={Stochastic Superoptimization},
      author={Eric Schkufza and Rahul Sharma and Alex Aiken},
      year={2012},
      eprint={1211.0557},
      archivePrefix={arXiv},
      primaryClass={cs.PF}
}

@misc{parisotto2016neurosymbolic,
      title={Neuro-Symbolic Program Synthesis},
      author={Emilio Parisotto and Abdel-rahman Mohamed and Rishabh Singh and Lihong Li and Dengyong Zhou and Pushmeet Kohli},
      year={2016},
      eprint={1611.01855},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@book{Koza92,
  added-at = {2008-11-22T15:57:31.000+0100},
  address = {Cambridge, MA, USA},
  author = {Koza, John R.},
  biburl = {https://www.bibsonomy.org/bibtex/27573e564bc5369e1a853e74b3ac62607/emanuel},
  interhash = {e8307fb6cf4ee27405142256d98c4c9e},
  intrahash = {7573e564bc5369e1a853e74b3ac62607},
  isbn = {0-262-11170-5},
  keywords = {enumerative_ip gp induction inductive_programming program_evolution program_synthesis},
  publisher = {MIT Press},
  timestamp = {2008-11-22T15:57:31.000+0100},
  title = {Genetic Programming: {O}n the Programming of Computers by Means of Natural Selection},
  year = 1992
}


@article{ref7,
  title={Explaining and Harnessing Adversarial Examples.},
  author={Ian J. Goodfellow and
               Jonathon Shlens and
               Christian Szegedy.},
  year={2015},
  journal={In ICLR}
}



@article{ref15,
  title={Adversarial examples in the physical world.},
  author={Alexey Kurakin and
               Ian J. Goodfellow and
               Samy Bengio.},
  year={2017},
  journal={In ICLR }
}

@article{ref17,
  title={Exploring the Space of Adversarial Images.},
  author={Pedro Tabacof and
               Eduardo Valle.},
  year={2016},
  journal={In IJCNN}
}

@article{ref29,
  title={Adversarial Examples: Attacks and Defenses for Deep Learning.},
  author={Xiaoyong Yuan and
               Pan He and
               Qile Zhu and
               Xiaolin Li},
  year={2019},
  journal={In {IEEE} Trans. Neural Networks Learn. Syst}
}

@article{ref56,
 title={Towards Deep Learning Models Resistant to Adversarial Attacks},
 author={Aleksander Madry and
               Aleksandar Makelov and
               Ludwig Schmidt and
               Dimitris Tsipras and
               Adrian Vladu},
 journal={In ICLR},
 year={2018}
}

@misc{VANILLAGRADIENT,
title = {Deep inside convolutional networks: Visualising image classification models and saliency maps.},
author = {Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman},
year={2013},
eprint={1312.6034},
archivePrefix={arXiv}
}

@article{MIPVERIFY,
 title={Evaluating Robustness of Neural Networks with Mixed Integer Programming},
 author={Vincent Tjeng, Kai Xiao, Russ Tedrake},
 journal={ICLR},
 year={2019}
}

@article{MARVEL,
 title={Maximal Robust Neural Network Specifications via Oracle-guided Numerical Optimization},
 author={Anan Kabaha and Dana Drachsler Cohen},
 journal={VMCAI},
 year={2023}
}

@article{PGD,
title={Towards Deep Learning Models Resistant to Adversarial Attacks},
author = {Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu},
journal={ICLR},
year={2018}
}

@article{ABSTRACTINTER,
title={An abstract domain for certifying neural networks},
author = {Gagandeep Singh, Timon Gehr, Markus Püschel, Martin Vechev},
journal={ACM},
year={2019}
}

@misc{COMPLETE,
title={Fast and Complete: Enabling Complete Neural Network Verification with Rapid and Massively Parallel Incomplete Verifiers},
author = {Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, Cho-Jui Hsieh},
journal={ICLR},
year={2020},
month={11},
eprint={2011.13824},
archivePrefix={arXiv}
}

@article{INCOMPLETE1,
title={Safety and Robustness Certification of Neural Networks with Abstract Interpretation},
author = {Timon Gehr; Matthew Mirman; Dana Drachsler-Cohen; Petar Tsankov; Swarat Chaudhuri; Martin Vechev},
journal={IEEE},
year={2018}
}

@article{INCOMPLETE2,
title={Fast and Effective Robustness Certification},
author = {Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Püschel, Martin Vechev},
journal={NeurIPS},
year={2018}
}

@article{SINGLElABEL1,
title={Threat of adversarial attacks on deep learning in computer vision: A survey},
author = {Naveed Akhtar and Ajmal Mian},
journal={IEEE},
year={2018}
}

@article{SINGLElABEL2,
title={Towards evaluating the robustness of neural networks},
author = {Nicholas Carlini and David Wagner},
journal={IEEE},
year={2017}
}

@article{SINGLElABEL3,
title={Explaining and harnessing adversarial examples},
author = {Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy},
journal={International Conference on Learning Representations},
year={2015}
}

@article{SINGLElABEL4,
title={Deepfool: a simple and accurate method to fool deep neural networks},
author = {Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard},
journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
year={2016}
}

@article{MULTIlABEL1,
title={Multi-label Adversarial Perturbations},
author={Qingquan Song; Haifeng Jin; Xiao Huang; Xia Hu},
year={2018},
journal={IEEE}
}

@INPROCEEDINGS{MULTIlABEL2,
  author={Zhou, Nan and Luo, Wenjian and Lin, Xin and Xu, Peilan and Zhang, Zhenya},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  title={Generating Multi-label Adversarial Examples by Linear Programming},
  year={2020},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/IJCNN48605.2020.9206614}}

@article{MULTIlABEL3,
    title={Characterizing the Evasion Attackability of Multi-label Classifiers},
    volume={35},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/17273},
    DOI={10.1609/aaai.v35i12.17273},
    abstractNote={Evasion attack in multi-label learning systems is an interesting, widely witnessed, yet rarely explored research topic. Characterizing the crucial factors determining the attackability of the multi-label adversarial threat is the key to interpret the origin of the adversarial vulnerability and to understand how to mitigate it. Our study is inspired by the theory of adversarial risk bound. We associate the attackability of a targeted multi-label classifier with the regularity of the classifier and the training data distribution. Beyond the theoretical attackability analysis, we further propose an efficient empirical attackability estimator via greedy label space exploration. It provides provably computational efficiency and approximation accuracy. Substantial experimental results on real-world datasets validate the unveiled attackability factors and the effectiveness of the proposed empirical attackability indicator.},
    number={12},
    journal={Proceedings of the AAAI Conference on Artificial Intelligence},
    author={Yang, Zhuo and Han, Yufei and Zhang, Xiangliang},
    year={2021},
    month={May},
    pages={10647-10655} }

@article{IMAGETAGGING,
title={Fast Image Tagging},
author={Minmin Chen, Alice Zheng, Kilian Weinberger},
    journal={Proceedings of the 30th International Conference on Machine Learning},
    year={2013},
}

@article{ObjectDetection,
title={A Trainable System for Object Detection},
author={Papageorgiou, C., Poggio, T},
    journal={International Journal of Computer Vision},
    year={2000},
}

@article{FacialRec,
title={Multiple Transfer Learning and Multi-Label Balanced Training Strategies for Facial AU Detection in the Wild},
author={Sijie Ji, Kai Wang, Xiaojiang Peng, Jianfei Yang, Zhaoyang Zeng, Yu Qiao},
    journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    year={2020},
}

@misc{MultiVul1,
title={Adversarial Extreme Multi-label Classification},
author={Rohit Babbar, Bernhard Schölkopf},
    year={2018},
eprint={1803.01570},
archivePrefix={arXiv}
}

@article{MultiVul2,
title={MultiGuard: Provably Robust Multi-label Classification against Adversarial Examples},
author={Jinyuan Jia, Wenjie Qu, Neil Gong},
    year={2022},
journal={Advances in Neural Information Processing Systems}
}

@article{MultiVul3,
title={Domain Knowledge Alleviates Adversarial Attacks in Multi-Label Classifiers},
author={Stefano Melacci, Gabriele Ciravegna, Angelo Sotgiu, Ambra Demontis, Battista Biggio, Marco Gori},
    year={2022},
journal={IEEE}
}

@article{OracleGuided,
title={Oracle-guided component-based program synthesis},
author={Jha, S., Gulwani, S., Seshia, S.A., Tiwari, A.},
    year={2010},
journal={ICSE}
}

@article{DoubleMNIST,
title={Dynamic Routing Between Capsules},
author={Jha, S., Gulwani, S., Seshia, S.A., Tiwari, A.Sara Sabour, Nicholas Frosst, Geoffrey E. Hinton},
    year={2017},
journal={NeurIPS}
}

@article{L0,
title={Adversarial Training Against Location-Optimized Adversarial Patches},
author={Sukrut Rao, David Stutz, Bernt Schiele},
    year={2020},
journal={ECCV}
}

@ARTICLE{CEGIS1,
  author={Li, Changjiang and Ji, Shouling and Weng, Haiqin and Li, Bo and Shi, Jie and Beyah, Raheem and Guo, Shanqing and Wang, Zonghui and Wang, Ting},
  journal={IEEE Transactions on Dependable and Secure Computing},
  title={Towards Certifying the Asymmetric Robustness for Neural Networks: Quantification and Applications},
  year={2022},
  volume={19},
  number={6},
  pages={3987-4001},
  doi={10.1109/TDSC.2021.3116105}}

@InProceedings{CEGIS2,
author="Kabaha, Anan
and Drachsler-Cohen, Dana",
editor="Singh, Gagandeep
and Urban, Caterina",
title="Boosting Robustness Verification of Semantic Feature Neighborhoods",
booktitle="Static Analysis",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="299--324",
abstract="Deep neural networks have been shown to be vulnerable to adversarial attacks that perturb inputs based on semantic features. Existing robustness analyzers can reason about semantic feature neighborhoods to increase the networks' reliability. However, despite the significant progress in these techniques, they still struggle to scale to deep networks and large neighborhoods. In this work, we introduce VeeP, an active learning approach that splits the verification process into a series of smaller verification steps, each is submitted to an existing robustness analyzer. The key idea is to build on prior steps to predict the next optimal step. The optimal step is predicted by estimating the robustness analyzer's velocity and sensitivity via parametric regression. We evaluate VeeP on MNIST, Fashion-MNIST, CIFAR-10 and ImageNet and show that it can analyze neighborhoods of various features: brightness, contrast, hue, saturation, and lightness. We show that, on average, given a 90 minute timeout, VeeP verifies 96{\%} of the maximally certifiable neighborhoods within 29 minutes, while existing splitting approaches verify, on average, 73{\%} of the maximally certifiable neighborhoods within 58 minutes.",
isbn="978-3-031-22308-2"
}


@InProceedings{CEGIS3,
  title = 	 {On Certifying Non-Uniform Bounds against Adversarial Attacks},
  author =       {Liu, Chen and Tomioka, Ryota and Cevher, Volkan},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4072--4081},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/liu19h/liu19h.pdf},
  url = 	 {https://proceedings.mlr.press/v97/liu19h.html},
  abstract = 	 {This work studies the robustness certification problem of neural network models, which aims to find certified adversary-free regions as large as possible around data points. In contrast to the existing approaches that seek regions bounded uniformly along all input features, we consider non-uniform bounds and use it to study the decision boundary of neural network models. We formulate our target as an optimization problem with nonlinear constraints. Then, a framework applicable for general feedforward neural networks is proposed to bound the output logits so that the relaxed problem can be solved by the augmented Lagrangian method. Our experiments show the non-uniform bounds have larger volumes than uniform ones. Compared with normal models, the robust models have even larger non-uniform bounds and better interpretability. Further, the geometric similarity of the non-uniform bounds gives a quantitative, data-agnostic metric of input features’ robustness.}
}



