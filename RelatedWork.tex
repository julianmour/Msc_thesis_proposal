%! Author = julianmour
%! Date = 01/05/2023

\section{Related Work}
Our thesis topics mainly involve Neural-Networks Verifiers, Adversarial Attacks in Multi-Label Classification and Counter Example Guided Synthesis.
We next review some related work.
\subsection{Neural-Networks Verifiers}
Neural network verifiers are tools or methods that are used to ensure the robustness of neural networks.
These verifiers use mathematical techniques such as constraint solving and model checking to analyze the behavior of neural networks and ensure that they operate correctly under certain kinds of adversarial attacks.
There are various neural network verification techniques, such as abstract interpretation (e.g.~\cite{ABSTRACTINTER, INCOMPLETE1}), linear programming (e.g.~\cite{MIPVERIFY}) and more.
Verifiers are usually divided into two categories:
\begin{itemize}
    \item Incomplete Verifiers - an incomplete verifier may only be able to prove the absence of adversarial examples for a subset of the inputs (not all of them)~\cite{INCOMPLETE1, INCOMPLETE2}.
    \item Complete Verifiers - a complete verifier can prove the absence of adversarial examples for all inputs~\cite{MIPVERIFY, COMPLETE}, but is likely more time expensive than other incomplete verifiers.
\end{itemize}
In our preliminary work we used $MIPVerify$, a MILP (Mixed Integer Linear Programming) verifier~\cite{MIPVERIFY} which is a complete verifier.

\subsection{Adversarial Attacks in Multi-Label Classification}
Adversarial attacks in multi-label classification refer to the phenomenon where an adversary intentionally modifies the input data to a multi-label classifier in order to cause misclassification or mislabeling of the input.
In multi-label classification, each input can be assigned multiple labels, and the classifier is trained to predict a subset of these labels for each input.
Adversarial attacks in this context can take various forms, such as adding perturbations to the input data to cause the classifier to predict any different incorrect subset of labels, or to cause the classifier to not predict a specific correct label.
We focus on the latter.
Most existing works on adversarial examples have been focused on the case of multi-class single-label classification~\cite{SINGLElABEL1, SINGLElABEL2, SINGLElABEL3, SINGLElABEL4}, but few on the case of multi-label classification~\cite{MULTIlABEL}.

\subsection{Counter Example Guided Synthesis}
Counter example guided synthesis (CEGIS) is a technique used in formal verification and program synthesis to generate correct programs or system designs from specific given specifications.
The basic idea behind CEGIS is to iteratively search for a candidate that satisfies the specification, while using counterexamples to refine the search space and guide the synthesis process.
In this thesis we use CEGIS to find the desired epsilons sequence that will define a robust maximal layer-neighborhood;
Our specification - a maximal robust layer-neighborhood, the counterexamples - adversarial examples and weakest points.
We submit several queries to a verifier in each iteration and update the epsilons sequence accordingly, given the counterexamples.
Previous work also used CEGIS to find maximal robust neighborhoods~\cite{MARVEL}, we use similar approach as this work.