%! Author = julianmour
%! Date = 01/05/2023

\section{Related Work}
Our thesis is related to neural network verification, adversarial attacks against multi-label classifiers, and counterexample-guided synthesis.

\subsection{Neural Network Verification}\label{subsec:verifiers}
Neural network verifiers analyze safety properties of neural networks.
These verifiers rely on different mathematical techniques, such as constraint solving and model checking, to analyze the behavior of neural networks and determine whether the given property holds.
There are various neural network verification techniques, such as abstract interpretation (e.g.~\cite{ABSTRACTINTER, INCOMPLETE1}), linear programming (e.g.~\cite{MIPVERIFY}) and more\Dana{add more, see Marvel}.
Two main categories of verifiers are:
\begin{itemize}
    \item Complete verifiers: return a definite answer whether a given property, such as neighborhood robustness, holds or not~\cite{MIPVERIFY, COMPLETE}. Such verifiers tend to have long execution time and thus do not scale to large networks.
        In our preliminary research, we rely on a complete verifier~\cite{MIPVERIFY}.
    \item Incomplete verifiers: may also return \emph{unknown} for a given property. This allows the, to rely on approximate techniques which scale better~\cite{INCOMPLETE1, INCOMPLETE2}.
\end{itemize}

\subsection{Adversarial Attacks in Multi-Label Classification}
In an adversarial attack, an adversary intentionally perturbs an input to the classifier to cause misclassification.
In multi-label classification, each input can be assigned multiple classes.
Adversarial attacks in this setting can be defined with respect to various goals, such as causing the classifier to predict any different incorrect subset of classes or not predicting a given correct class.
We focus on the latter goal, also known as untargeted attack.
Most existing works on adversarial attacks have been focused on the case of multi-class single-label classification~\cite{SINGLElABEL1, SINGLElABEL2, SINGLElABEL3, SINGLElABEL4}.\Dana{is the previous sentence related to us? we are in multi label no?}
%Several others on the case of multi-label classification, where attacks are mainly divided to two types: \textbf{(1)~Targeted Attacks} - aim to bring specific incorrect labels' scores to be the highest and \textbf{(2)~Untargeted Attacks} - aim to bring the correct labels' scores to not be the highest.
%Our property focuses on giving a robust neighborhood against a type of untargeted attacks;
%Such attacks that aim to bring a specific correct label's score to not be in the top picked labels.
    Song et al.~\cite{MULTIlABEL1} introduce targeted white-box attacks for multi-label classification.
    They approach the problem by formulating it as an optimization problem and then execute gradient descent.
    Through experimentation, they discover that they could manipulate a multi-label classifier into producing any set of labels for a given input by adding an adversarial perturbation\Dana{what is the discovery? sounds like a solution to their optimization function}.
    Zhou et al.~\cite{MULTIlABEL2} suggest generating $L_{\infty}$-norm adversarial perturbations to trick multi-label classifiers.
    They solve the problem by transforming the optimization problem of finding adversarial perturbations into a linear programming problem, which can be solved efficiently.
    Yang et al.~\cite{MULTIlABEL3} explore the potential for misclassification risk in multi-label classifiers, particularly in worst-case scenarios.
    They approach the problem by formulating it as a bi-level set function optimization problem and use random greedy search to find an approximate solution.
    \Dana{there are only three works? we need at least six and ideally ten if there are}

\subsection{Counterexample-Guided Synthesis}
Counter example guided synthesis (CEGIS) is a technique used in formal verification and program synthesis to generate correct programs or system designs for given specifications.
CEGIS iteratively searches for a candidate, checks with an oracle whether the candidate satisfies the specification, and if not relies on counterexamples to refine the search space and guide the synthesis process.
In this research, we use CEGIS to recover the search for a robust neighborhood after reaching a non-robust neighborhood. %desired epsilons sequence that will define a robust maximal layer-neighborhood;
In our context, the specification is a robust layer-neighborhood, the oracle is the verifier, and the counterexamples are the weakest points.
%We submit several queries to a verifier in each iteration and update the epsilons sequence accordingly, given the counterexamples.
Previous work also used CEGIS to find maximal robust neighborhoods~\cite{CEGIS1, MARVEL, CEGIS2, CEGIS3}. %, we use similar approach as MaRVeL's~\cite{MARVEL}. 